Module 2: Appreciating, Interpreting and Visualizing Data
Lab 4: t-Distributed Stochastic Neighbor Embedding (t-SNE)
Coordinator: Aswin Jose
This lab will be based upon t-SNE which is a dimensionality reduction algorithm used to visualize high dimensional datasets.

t-SNE stands for t-Distributed Stochastic Neighbor Embedding. It is an unsupervised, non-linear technique primarily used for data exploration and visualizing high-dimensional data. It was developed by Laurens van der Maatens and Geoffrey Hinton in 2008 (Link to the paper: https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)

image.png

t-SNE has a tuneable parameter, perplexity which balances attention between the local and global aspects of your data. It is a guess about the number of close neighbors each point has. The perplexity value has a complex effect on the resulting pictures. The original paper says, “The performance of t-SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50.” But the story is more nuanced than that. Getting the most from t-SNE may mean analyzing multiple plots with different perplexities.

Also the t-SNE algorithm doesn’t always produce similar output on successive runs as there are additional hyperparameters related to the optimization process.

HOW DOES T-SNE WORK??
The t-SNE algorithm calculates a similarity measure between pairs of instances in the high dimensional space and in the low dimensional space. It then tries to optimize these two similarity measures using a cost function. Let’s break that down into 3 basic steps.

Measure similarities between points in the high dimensional space. Think of a bunch of data points scattered on a 2D space. For each data point (xi) we’ll center a Gaussian distribution over that point. Then we measure the density of all points (xj) under that Gaussian distribution. Then renormalize for all points. This gives us a set of probabilities (Pij) for all points. Those probabilities are proportional to the similarities. All that means is, if data points x1 and x2 have equal values under this gaussian circle then their proportions and similarities are equal and hence you have local similarities in the structure of this high-dimensional space. The Gaussian distribution or circle can be manipulated using what’s called perplexity, which influences the variance of the distribution (circle size) and essentially the number of nearest neighbors.

This step is similar to step 1, but instead of using a Gaussian distribution we use a Student t-distribution with one degree of freedom, which is also known as the Cauchy distribution (See fig below). This gives us a second set of probabilities (Qij) in the low dimensional space. As you can see the Student t-distribution has heavier tails than the normal distribution. The heavy tails allow for better modeling of far apart distances.

The last step is that we want these set of probabilities from the low-dimensional space (Qij) to reflect those of the high dimensional space (Pij) as best as possible. We want the two map structures to be similar. We measure the difference between the probability distributions of the two-dimensional spaces using Kullback-Liebler divergence (KL). Finally, we use gradient descent to minimize our KL cost function.

students_t.jpeg

Scikit-learn has an implementation of t-SNE available which provides a wide variety of tuning parameters for t-SNE, and the most notable ones are:

n_components (default: 2): Dimension of the embedded space.
perplexity (default: 30): The perplexity is related to the number of nearest neighbors that are used in other manifold learning algorithms. Consider selecting a value between 5 and 50.
n_iter (default: 1000): Maximum number of iterations for the optimization. Should be at least 250.
method (default: ‘barnes_hut’): Barnes-Hut approximation runs in O(NlogN) time. method=’exact’ will run on the slower, but exact, algorithm in O(N^2) time.

[ ]
# numpy.
import numpy as np
from numpy import linalg
from numpy.linalg import norm
from scipy.spatial.distance import squareform, pdist

# sklearn.
import sklearn
from sklearn.manifold import TSNE
from sklearn.datasets import load_digits
from sklearn.preprocessing import scale

# Random state.
RS = 20150101

# matplotlib.
import matplotlib.pyplot as plt
import matplotlib.patheffects as PathEffects
import matplotlib
%matplotlib inline

# seaborn.
import seaborn as sns
sns.set_style('darkgrid')
sns.set_palette('muted')
sns.set_context("notebook", font_scale=1.5,
                rc={"lines.linewidth": 2.5})
1797 images each of size 8 x 8 loaded using load_digits()


[ ]
digits = load_digits()
digits.data.shape
(1797, 64)
Printing some images from the dataset


[ ]
nrows, ncols = 2, 5
plt.figure(figsize=(6,3))
plt.gray()
for i in range(ncols * nrows):
    ax = plt.subplot(nrows, ncols, i + 1)
    ax.matshow(digits.images[i,...])
    plt.xticks([]); plt.yticks([])
    plt.title(digits.target[i])


[ ]
# We first reorder the data points according to the handwritten numbers.
X = np.vstack([digits.data[digits.target==i]
               for i in range(10)])
y = np.hstack([digits.target[digits.target==i]
               for i in range(10)])
Now using TSNE to fit the dataset with the default values.
n_components : 2
perplexity : 30
n_iter : 1000
method : ‘barnes_hut’

[ ]
digits_proj = TSNE(init="pca", random_state=RS).fit_transform(X)
Visualizing the data in the projected space

[ ]
def scatter(x, colors):
    # We choose a color palette with seaborn.
    palette = np.array(sns.color_palette("hls", 10))

    # We create a scatter plot.
    f = plt.figure(figsize=(8, 8))
    ax = plt.subplot(aspect='equal')
    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,
                    c=palette[colors.astype(int)])
    plt.xlim(-25, 25)
    plt.ylim(-25, 25)
    ax.axis('off')
    ax.axis('tight')

    # We add the labels for each digit.
    txts = []
    for i in range(10):
        # Position of each label.
        xtext, ytext = np.median(x[colors == i, :], axis=0)
        txt = ax.text(xtext, ytext, str(i), fontsize=24)
        txt.set_path_effects([
            PathEffects.Stroke(linewidth=5, foreground="w"),
            PathEffects.Normal()])
        txts.append(txt)
    plt.show()

    return f, ax, sc

scatter(digits_proj, y)

Tweaking some of the hyperparameters to better understand their role
Changing the PERPLEXITY values
image.png

With perplexity values in the range (5 - 50) suggested by van der Maaten & Hinton, the diagrams do show these clusters, although with very different shapes. Outside that range, things get a little weird. With perplexity 2, local variations dominate. The image for perplexity 100, with merged clusters, illustrates a pitfall: for the algorithm to operate properly, the perplexity really should be smaller than the number of points. Implementations can give unexpected behavior otherwise.

n_components : 2
perplexity : 5
n_iter : 1000
method : ‘barnes_hut’

[ ]
digits_proj = TSNE(init="pca", random_state=RS, perplexity=5).fit_transform(X)

scatter(digits_proj, y)

We can see that there are local clusters within the same number group as well. This is happening as the perplexity being at 5, allows the local neighbourhood to dominate. Let us now see what happens if we increase the perplexity to 100, thereby increasing global impact.

n_components : 2
perplexity : 100
n_iter : 1000
method : ‘barnes_hut’

[ ]
digits_proj = TSNE(init="pca", random_state=RS, perplexity=100).fit_transform(X)

scatter(digits_proj, y)

The general structure of the plot remained similar to the one with perplexity = 30 (default), but on careful observation you can observe many data points not being part of the group they are supposed to be in. This is because of the large number of points considered for the neighbourhood (as perplexity value = 100 is higher), thereby allowing 2 data points from different groups to end up closer.

Changing the NUMBER OF ITERATIONS
image.png

The images above show five different runs at perplexity 30. The first four were stopped before stability. After 10, 20, 60, and 120 steps you can see layouts with seeming 1-dimensional and even pointlike images of the clusters. If you see a t-SNE plot with strange “pinched” shapes, chances are the process was stopped too early. Unfortunately, there’s no fixed number of steps that yields a stable result. Different data sets can require different numbers of iterations to converge.

The most important thing is to iterate until reaching a stable configuration.

n_components : 2
perplexity : 30
n_iter : 250
method : ‘barnes_hut’

[ ]
digits_proj = TSNE(init="pca", random_state=RS, n_iter=250).fit_transform(X)

scatter(digits_proj, y)

As can be seen from the figure above, stopping the optimization earlier (in 250 iterations) resulted in a suboptimal clustering of the groups.

Let us now see how the results are affected if t-SNE is run for larger number of iterations

n_components : 2
perplexity : 30
n_iter : 5000
method : ‘barnes_hut’

[ ]
digits_proj = TSNE(init="pca", random_state=RS, n_iter=5000).fit_transform(X)

scatter(digits_proj, y)

Running for larger number of iterations more or less resulted in the same plot as the optimization had nearly converged till the default 1000 iterations. However the density of the clusters has increased.

Changing the method to EXACT
The barnes-hut method takes O(NlogN) time, whereas the exact method takes O(N^2) time. Notice the increase in the execution time while running the cell below.

n_components : 2
perplexity : 30
n_iter : 1000
method : ‘exact’

[ ]
digits_proj = TSNE(init="pca", random_state=RS, method='exact').fit_transform(X)

scatter(digits_proj, y)

EXERCISE
Try out different perplexity and iteration values to better appreciate the concepts taught.

You can modify the code in the cell below to check the resultant plots.


[ ]
###############################################################
###### MODIFY THE VALUES FOR THE HYPERPARAMETERS BELOW ########

perplexity_value = 2
number_iterations = 1000
method = "barnes_hut"

###############################################################

digits_proj = TSNE(init="pca", random_state=RS, n_iter=number_iterations, perplexity=perplexity_value, method=method).fit_transform(X)
scatter(digits_proj, y)

Exercise:
Now that you understand t-SNE a bit better, can you point out some differences between PCA and t-SNE. What are the advantages/disadvantages of one over the other?
Pointers:
a. Which of the two algorithms is linear and which one is non-linear?
When comparing **linear** and **non-linear** algorithms, the distinction generally refers to whether the algorithm makes decisions based on a linear relationship between the input variables and the target variable. Here’s how they apply to some common algorithms:

### 1. **Linear Algorithms**
A **linear algorithm** assumes a linear relationship between input features and the output. These algorithms attempt to model data using a straight line or a hyperplane (in higher dimensions). The decision boundary is linear in these cases.

- **Principal Component Analysis (PCA)**:  
  PCA is **linear** because it identifies the directions (principal components) in the data that capture the most variance. The transformation is based on linear combinations of the original features. It uses linear algebra (e.g., eigenvalues and eigenvectors) to find the principal components.

- **Linear Regression**:  
  This is another linear algorithm. The model assumes a linear relationship between the input variables (features) and the output (target). It fits a line (or hyperplane) to the data.

- **Logistic Regression** (when applied to a linear decision boundary):  
  Though logistic regression uses the logistic function, if we consider its behavior in a basic form, it is linear with respect to the features in the decision boundary. The model assumes a linear relationship between the input variables and the log-odds of the target.

### 2. **Non-Linear Algorithms**
A **non-linear algorithm** does not assume a linear relationship between input variables and the target variable. These models can capture complex, non-linear relationships between features and the target.

- **Kernel Principal Component Analysis (Kernel PCA)**:  
  **Kernel PCA** is **non-linear** because it uses a kernel function (such as Gaussian, polynomial, etc.) to map the data into a higher-dimensional feature space where a linear decision boundary can be used. However, the decision boundary in the original feature space is non-linear. The transformation is done non-linearly using the kernel trick.

- **Support Vector Machines (SVM) with Non-Linear Kernel**:  
  SVM itself can be both linear or non-linear depending on the kernel used. If a **linear kernel** is used, it is a linear algorithm, but if a **non-linear kernel** (e.g., RBF or polynomial) is used, the SVM becomes non-linear.

- **Decision Trees**:  
  Decision trees are inherently **non-linear**. They make decisions based on splitting the data at different feature values, and these splits do not have to follow a linear pattern.

- **Neural Networks**:  
  Neural networks, particularly deep learning models, are also **non-linear**. They use layers of interconnected nodes (neurons), with non-linear activation functions (like ReLU or sigmoid), allowing them to model highly complex relationships.

### Summary:
- **Linear Algorithm**: **PCA (Principal Component Analysis)** is linear.
- **Non-Linear Algorithm**: **Kernel PCA (Kernel Principal Component Analysis)** is non-linear.
b. How does the non-linearity in one of these two algorithms help in capturing certain data sets?
The non-linearity in algorithms like **Kernel PCA (Kernel Principal Component Analysis)** plays a significant role in capturing complex relationships within certain datasets, particularly when the data cannot be separated or represented well in its original feature space using linear methods. Here's how the non-linearity helps in capturing certain datasets:

### 1. **Understanding Non-Linearity in Kernel PCA**:
   - **PCA** is a linear technique that projects the data into the directions of maximum variance in a linear manner. However, PCA struggles when the data lies in a non-linear space or has a complex structure that a linear transformation cannot capture effectively.
   - **Kernel PCA** overcomes this limitation by using the **kernel trick**, which implicitly maps the data into a higher-dimensional feature space where the relationships between the data points may be more easily separated or captured linearly.

### 2. **How Non-Linearity Helps in Kernel PCA**:
   
   In **Kernel PCA**, the non-linearity comes from the kernel function, which transforms the data from its original space to a higher-dimensional space without explicitly computing the mapping. This non-linear transformation allows the algorithm to capture more complex relationships that might not be apparent in the original feature space.

   - **Kernel Trick**: Instead of directly performing a linear transformation in the original space, Kernel PCA computes a similarity matrix based on the kernel function (like the **Gaussian kernel** or **polynomial kernel**) that implicitly defines the higher-dimensional feature space.
   
   - **Separation in Higher Dimensions**: In this higher-dimensional feature space, the relationships between data points may become linear. For example, complex data structures like spirals, concentric circles, or data with non-linear boundaries in the original space can become separable after the transformation.
   
### 3. **Examples Where Non-Linearity Helps**:

   Consider these scenarios where non-linearity makes a big difference in capturing the data's structure:

   - **Spiral or Circular Data**: 
     In a 2D plane, data arranged in a spiral or concentric circles might not be linearly separable. However, after applying a kernel (e.g., RBF or polynomial), Kernel PCA projects the data into a higher-dimensional space where it may form clusters or separations that are easier to capture using linear techniques.
     - **Without Kernel PCA**: A linear method like PCA would struggle to identify the true structure of the spiral or circular data since it’s inherently non-linear.
     - **With Kernel PCA**: The non-linear transformation maps the data in such a way that the spiral or circular patterns may become more linear, and thus separable.

   - **Non-Linear Boundaries in Classification**: 
     If you are working with a classification task where classes are distributed in a non-linear manner (e.g., classes with complex decision boundaries), Kernel PCA allows the data to be mapped into a higher-dimensional space where a simpler linear decision boundary can separate the classes.
     - **Without Kernel PCA**: Linear classifiers like Logistic Regression or Linear SVM may fail to capture the non-linear decision boundary and might perform poorly.
     - **With Kernel PCA**: A Kernel SVM with a non-linear kernel (like RBF) can easily find a non-linear boundary after the Kernel PCA transformation, improving classification accuracy.

   - **Complex, High-Dimensional Data**: 
     In higher-dimensional spaces (such as when working with images or text data), the relationships between features are often highly complex and not linear. Kernel PCA helps by mapping the data into a higher-dimensional space where linear methods like PCA or linear SVMs can more easily model the data’s intrinsic structure.
     - **Without Kernel PCA**: The linear transformation provided by traditional PCA or linear algorithms might not be able to capture the complex relationships present in high-dimensional data.
     - **With Kernel PCA**: By leveraging non-linearity, Kernel PCA allows for better dimensionality reduction or separation of complex patterns in the data.

### 4. **Practical Example**: 
Consider a simple example with two features: if the data forms a **non-linear pattern** (e.g., a concentric circle or a spiral shape), traditional PCA will attempt to reduce dimensionality in a straight line and won't be able to represent the underlying structure of the data. However, Kernel PCA will map the data into a higher-dimensional space, where this non-linear structure might become linear, allowing the components to capture the variance in the transformed data much better.

### 5. **Advantages of Non-Linearity in Kernel PCA**:
   - **Capturing Complex Patterns**: Non-linearity allows Kernel PCA to capture more complex patterns, such as curved or circular relationships, that are difficult to detect with linear methods.
   - **Higher Accuracy in Tasks**: In tasks such as classification, clustering, or regression, non-linear techniques can provide higher accuracy when the data has intricate and non-linear patterns.
   - **Flexibility**: The kernel trick is highly flexible since different kernels (e.g., polynomial, Gaussian/RBF, etc.) can be applied depending on the type of data and the relationships you want to capture.

### 6. **Conclusion**:
   Non-linearity in algorithms like Kernel PCA helps capture complex data relationships that cannot be modeled linearly. By transforming the data into a higher-dimensional feature space through a kernel function, Kernel PCA allows linear methods to be applied effectively to non-linearly separable data. This is particularly helpful for datasets with non-linear decision boundaries, spirals, or circular patterns, and in high-dimensional spaces where the relationships between features are intricate and complex.
c. PCA is known to keep points which were further apart in the higher dimension, far apart in the lower dimension as well. Does t-SNE do the same? Or does it try to preserve local neighbourhood?
t-SNE (t-Distributed Stochastic Neighbor Embedding) does **not** aim to preserve the distances between points in the same way PCA does. While PCA focuses on capturing the **global structure** of the data (i.e., trying to preserve the variance and the relative distances between points in the higher-dimensional space), t-SNE primarily focuses on **preserving local structure** and the **neighborhood relationships**.

### Key Differences Between PCA and t-SNE:

1. **PCA (Principal Component Analysis)**:
   - **Global Structure Preservation**: PCA is a **linear** dimensionality reduction technique that projects data into a lower-dimensional space by maximizing the variance. It attempts to retain the overall **global structure** of the data.
   - **Distances**: PCA tries to keep the **relative distances** between points in the lower-dimensional space that best reflect their distances in the higher-dimensional space. However, it does not always do a good job of preserving local relationships, particularly for complex data structures.
   - **Global Patterns**: PCA is good for data where global relationships and variance are most important, such as for datasets where the main components of variance are linear.

2. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**:
   - **Local Structure Preservation**: t-SNE is a **non-linear** dimensionality reduction technique designed specifically to preserve **local neighborhood relationships** in the data. It focuses on keeping similar points close to each other in the lower-dimensional space, while points that are distant in the original high-dimensional space may not be as closely related in the reduced space.
   - **Similarity-Based Embedding**: t-SNE works by first converting pairwise distances (or similarities) between points in the high-dimensional space into probabilities. The goal is to ensure that **points that are similar** in the original space (i.e., their pairwise distance is small) will also be similar in the lower-dimensional space. Dissimilar points, on the other hand, are less important.
   - **Non-linear Embedding**: Unlike PCA, t-SNE is **non-linear** and works by minimizing the **Kullback-Leibler (KL) divergence** between the probability distribution of pairwise similarities in the original space and the probability distribution of pairwise similarities in the reduced space. This means t-SNE focuses more on **local relationships** rather than global structure.

### Behavior of t-SNE vs PCA:
- **Preserving Local Neighborhoods**: t-SNE tends to group similar data points together, even if they were far apart in the original high-dimensional space. This makes it particularly useful for tasks like visualizing clusters or discovering underlying patterns in data.
- **Preserving Global Structure**: t-SNE is **not** great at preserving the global structure of the data. Distances between clusters in the reduced space may not reflect the distances in the high-dimensional space. In contrast, PCA tries to keep the global structure more intact.

### Summary:
- **PCA** focuses on preserving **global relationships**, meaning it tries to maintain the overall variance and distances between points in the lower-dimensional space.
- **t-SNE**, on the other hand, is more focused on preserving **local neighborhoods**, meaning that points that are close to each other in the high-dimensional space will tend to be close in the lower-dimensional space as well. However, it **does not preserve global distances** well, which can lead to distorted representations of larger structures.

### Use Cases:
- **PCA**: Ideal for visualizing and understanding the **global variance** in the data, and for linear relationships where the structure is relatively simple and global.
- **t-SNE**: Best suited for **clustering**, **pattern discovery**, or visualizing **highly complex and non-linear relationships** in data, where the local structure is more important than the global structure.


d. Can you comment on which one of the two is computationally more expensive?
When comparing the computational expense of **PCA** and **t-SNE**, **t-SNE** is generally **much more computationally expensive** than PCA. Below are the key reasons for the differences in their computational complexity:

### 1. **PCA (Principal Component Analysis)**:
- **Linear Computation**: PCA is a linear method that reduces dimensionality by projecting the data onto the directions of maximum variance. It does this by computing the **eigenvectors** and **eigenvalues** of the covariance matrix of the data, which is a relatively straightforward linear algebra problem.
  
- **Computational Complexity**: The primary cost of PCA is in calculating the **covariance matrix** (which is an \(O(n^2)\) operation for \(n\) data points) and then solving the **eigenvalue problem**, which involves eigen-decomposition. In general, the complexity for PCA is:
  
  \[
  O(n^2 \cdot d)
  \]
  where:
  - \(n\) is the number of samples (data points),
  - \(d\) is the number of dimensions (features).

  If you use **Singular Value Decomposition (SVD)** to compute the principal components, the complexity can be \(O(n \cdot d^2)\), which is still relatively efficient for datasets with large numbers of features but not a massive number of samples.

- **Efficiency**: For large datasets with high-dimensional features, PCA is highly efficient and often used as a preprocessing step for dimensionality reduction.

### 2. **t-SNE (t-Distributed Stochastic Neighbor Embedding)**:
- **Non-Linear Method**: t-SNE is a **non-linear** technique designed to capture local structure by modeling the similarities between data points. It works by first computing pairwise similarity probabilities between all data points and then minimizing the **Kullback-Leibler (KL) divergence** between the high-dimensional space and the low-dimensional embedding.

- **Computational Complexity**:
  - **Pairwise Distance Calculations**: t-SNE requires computing pairwise **similarities** between all points in the dataset. This involves calculating distances between all pairs of \(n\) points in the original \(d\)-dimensional space. The complexity for this step is \(O(n^2)\), where \(n\) is the number of samples.
  - **Optimization**: After calculating the pairwise similarities, t-SNE uses an optimization algorithm (usually **gradient descent**) to minimize the KL divergence, which can take a significant amount of time depending on the number of iterations and the number of points. The optimization process itself can have a time complexity of around \(O(n^2)\), making it computationally expensive, especially for large datasets.

- **Efficiency**: t-SNE is **slow** for large datasets because it needs to compute pairwise similarities between all points and then perform an optimization procedure. Even with approximation methods like **Barnes-Hut t-SNE**, the complexity can still be \(O(n \log n)\), which is an improvement but still slower than PCA.

### Comparison Summary:

| Aspect                    | **PCA**                                   | **t-SNE**                                |
|---------------------------|-------------------------------------------|------------------------------------------|
| **Type of Method**         | Linear (Global structure)                 | Non-linear (Local structure)             |
| **Computational Complexity**| \(O(n^2 \cdot d)\) for covariance matrix | \(O(n^2)\) (pairwise distances) or \(O(n \log n)\) (Barnes-Hut) |
| **Efficiency**             | Efficient for large datasets              | Computationally expensive, especially for large datasets |
| **Scalability**            | Scales well with high-dimensional data    | Struggles with large datasets (scaling is an issue) |
| **Best Use Case**          | High-dimensional data with linear relationships | Clustering and visualization with complex data |

### Conclusion:
- **PCA** is much more efficient computationally, particularly for datasets with many dimensions (high \(d\)), because its time complexity is largely dependent on the number of features and samples but is generally lower than t-SNE.
- **t-SNE** is **computationally expensive**, especially when dealing with large datasets. Its complexity grows quadratically with the number of samples (\(O(n^2)\)) and is slower to compute compared to PCA, which can be a bottleneck for large datasets.


How does the computational complexity and runtime of t-SNE scale with dataset size and dimensionality?

What are some limitations or potential pitfalls to be aware of when using t-SNE? (tell atleast 3)

Some interesting references:
https://blog.paperspace.com/dimension-reduction-with-t-sne/
https://towardsdatascience.com/an-introduction-to-t-sne-with-python-example-5a3a293108d1
https://distill.pub/2016/misread-tsne/
